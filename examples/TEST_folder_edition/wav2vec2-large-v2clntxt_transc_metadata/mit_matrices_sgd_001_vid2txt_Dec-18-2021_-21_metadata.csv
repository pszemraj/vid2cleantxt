orig_file,num_audio_chunks,chunk_len_sec,input_dur_mins,date_of_transc,full_text,num_chars,word_count
MIT_MatricesSGD_001.mp4,80,15,20.0,Dec-18-2021_-21,"TIMES SOME RANDOMLY CHOSEN DATA POINT YOU COMPUTE ITS GRADIENT THIS IS S G D THAT'S WHAT WE'RE RUNNING AND WE THREW AWAY TONS OF INFORMATION
 WE DIDN'T USE THE FULL GRADIENT WE ARE JUST USING THIS CRUDE CRUDE GRADIENT SO THIS PROCESS IS VERY SENSITIVE TO THE OTHER PAROMETER IN THE SYSTEM WHICH IS THE STEP SIZE MUCH MORE SENSITIVE THAN GRADIENT DESCENT IN FACT ANDYOU LET'S SEE AS
 VARY THE STEP SIZE SEE IF YOU CAN NOTICE SOME PARONS ON A HOW IT TRIES TO GO TOWARDS AN OPTIMU
 IS A ZUNDIN VERSION ALSO OF THIS LETTER HERE I' COME TOTH
 SHORTLY AGAIN I'LL REPEAT AGAIN AND THEN I'LL ASK YOU FOR YOUR OBSERVATIONS IF YOUD NOTICE SHEME PARENTS I DON'T KNOW IF THEY'RE NECSERLY APPARENT YOU KNOW THAT'S THE THING WITH PARENTS ECAUSE I KNOW THE ANSWER SO I'LL SEE THE PARENT IF YOUDO
 KNOW THE ANSWER YOU MAY OR MAY NOT SEE THE PATTERN BUT I WANT TO SEE IF YOU ACTUALLY SEE THE PATTERN AS I MOVE CHANGE THE STEP SIZE SO MAYBE THAT WAS ENOUGH SIMULATION ANYBODY HAVE ANY COMMENTS ON WHAT KIND OF PATTERN YOU MAY HAVE OBSERVED SEEMS I
 CLUSTER IN THE MIDDLE IS GETTING LARGER AND WY MORE WIDESPREAD YOUHAVE DEFINITELY THAT'S A GREAT OBSERVATION ANY OTHER COMMENTS THERE'S ONE MORE INTERESTING THING
 HAPPENING HERE WHICH IS A VERY VERY TYPICAL THING FOR S G D IN ONE OF THE REASONS WHY PEOPLE LOVE S G D LET ME DO THAT ONCE AGAIN BRIEFLY OK THIS IS TINY STEP SIZE ALMOST
 ZERO CLOSE TO ZERUS NOT EXACTLY ZERO SO YOU SEE WHAT HAPPENS FOR A VERY TINY STEP SIZE IT DOESN'T LOOK THAT STARCASTIC RIGHT BUT THAT'S KIND OF OBVIOUS FROM THERE IF ATAK IS VERY TINY
 YOU'LL HARDLY MAKE ANY MOVE SO THINGS WILL LOOK VERY STABLE AND IN FACT THE SPEED AT WHICH THE CASTI GRADIANT CONVERGES IS THATSWY EXTREMELY SENSITIVE TO HOW YOU PICK THE STEP SIDES IT'S STILL AN OPEN RESEARCH PROBLEM TO COME
 THE BEST WAY TO PICK STEP SIZES OKISITS EVEN THOUGH IT'S SIMPLE IT DOESN'T MEAN IT'S TRIVIAL AND AS I VARY THE STEP SIZE O IT MAKES SOME PROGRESS AND IT GOES TOWARDS THE SOLUTION ARE YOU NOW BEGINNING TO SAE THAT
 SEEMS TO BE MAKING A MORE STABLE PROGRESS IN THE BEGINNING AND WHEN IT COMES CLOSE TO THE SOLUTION IT'S FLUCTUATING MORE AND THE BIGGER THE STEP SIZE THE AMOUNT OF
 TUATION NEAR THE SOLUTION IS WILDER AS HE NOTICED BACK THERE BUT ONE VERY INTERESTING THING IS MORE OR LESS CONSTANT THERE IS MORE FLUCTUATION ALSO ON THE OUTSIDE BUT YOU SEE THAT THE INITIAL
 STILL SEEMS TO BE MAKING PRETTY GOOD PROGRESS AND AS YOU COME CLOSE TO THE SOLUTION IT FLUCTUATES MORE AND THAT IS A VERY PRINCIPALLY TYPICAL BEHAVIOR OF SARCASTIC GRADIENT DESCENT THAT IN THE BEGINNING IT MAKES
 RAPID STRIDES SO YOU MAY SEE YOUR TRAINING LAWS DECREASE WHOOP SUPERFAST AND THEN KIND OF INOPERA OUT AND IT'S THIS PARTICULAR BEHAVIOUR WHICH GOT PEOPLE SUPER EXCITED THAT HAY
 IN MACHINE LEARNING WE ARE WORKING WITH ALL SORTS OF BIG DATA I JUST WANT A QUICK AND DIRTY PROGRESS ON MY TRAINING TRAINING I DON'T CARE ABOUT GETTING TO THE BEST OPTIMUM BECAUSE IN MACHINE LEARNING YOU DON'T JUST CARE ABOUT SOLVING THE
 OPTIMIZATION PROBLEM YOU ACTUALLY CARE ABOUT FINDING SOLUTIONS THAT WORK WELL ON UNSEEN DATA SO THAT MEANS YOU DON'T WANT TO OVERFIT AND SOLVE THE OPTIMIZATION PROBLEM SUPREMELY WELL SO IT'S GREAT TO MAKE RAPID INITIAL PROGRESS AND IF
 THAT PROGRESS PEERS OUT IT'S O K THIS INTUITIONISTIC STATEMENTS THAT I'M MAKING IN SOME NICE CASES LIKE CONVEX OPTIMIZATION PROBLEMS ONE CAN MATHEMATICALLY FULLY QUANTIFY THESE ONE CAN PROVE THEOREMS TO
 FY EACH THING THAT I SAID IN TERMS OF HOW CLOSE HOW FAST AND SO ON WE'LL SEE A LITTLE BIT OF THAT AND THIS IS WHAT REALLY HAPPENS TO S G D YOU KNOW IT MAKES GREAT INITIAL PROGRESS AND REGARDLESS OF A
 HOW YOU USE STEP SIZES CLOSE TO THE OPTIMUM IT CAN EITHER GET STUCK OR ENTER SOME KIND OF CHAOS DYNAMICS OR JUST BEHAVE LIKE CRAZYO THAT'S TYPICAL OF S JIDDY AND LET'S LOOK AT NOW SLIGHT
 METICAL INSIGHT INTO ER ROUGHLY WHY THIS BEHAVIOR MAY HAPPEN THIS IS A TRIVIAL ONE DIMENSIONAL OPTIMIZATION PROBLEM BUT IT CONVEYS THE CRUX OF WHY THIS
 BEHAVIOR IS DISPLAYED BY SARCASTIC GRADIENT METHODS THAT IT WORKS REALLY WELL IN THE BEGINNING AND THEN IT CAN GOD KNOWS WHAT HAPPENS WHEN IT COMES CLOSE TO THE OPTIM ANYTHING CAN HAPPEN SO LET'S LOOK AT THAT
 O K SO LET'S LOOK AT A SIMPLE ONE DIMENSIONAL OPTIMIZATION PROBLEM I'LL KIND OF DRAW IT OUT MAYBE ON THE OTHER SIDE SO THAT PEOPLE ON THIS SIDE ARE NOT DISADVANTAGE
 SO I'LL JUST DRAW OUT A L SQUARES PROBLEM X IS ONE DIMENSIONAL PREVIOUSLY I HAD AI TRANSPOSE X NOW AI IS ALSO A SCALER SUJEST ONDY STUFF
 EVERYTHING IS ONE D SO THIS IS OUR SETUP THINK OF A I INTO X MINUS B I THESE ARE QUADRATIC FUNCTIONS RIGHT
 THEY LOOKED LIKE THIS CORRESPONDING TO DIFFERENT EYES THER'S LIKE SOME DIFFERENT FUNCTIONS SITTING AND SO ON SO
 ESE ARE MY NDIFFERENT LAWS FUNCTIONS AND I WANT TO MINIMIZE THOSE
 WE KNOW WE CAN ACTUALLY EXPLICITLY COMPUTE THE SOLUTION OF THAT PROBLEM RIGHT YOU SET SO YOU SET THE DERIVATIVE OF F OF X TO ZERO SO OR YOU SET THE GRADIENT OF F OF X TO ZERO HOPEFULLY
 THAT'S EASY FOR YOU TO DO IF YOU DO THAT DIFFERENTIATION WE'LL GET GRADIENT OF F OF X WELL BE JUST GIVEN BY WELL YOU CAN DO THAT IN YOUR HEAD I'LL JUST WRITE IT OUT EXPLICITLY A
 X MINUS B I TIMES A I IS EQUAL TO ZERO AND YOU SOLVE THAT FOR X YOU GET X STAR THE OPTIMUM OF THIS LEA SQUARE PROBLEM RIGHT
 SO WE ACTUALLY KNOW HOW TO SOLVE IT PRETTY EASILY IT'S A REALLY COOLD EXAMPLE ACTUALLY I GOT THAT FROM TEXT BOOK BY PROFESSOR DMITRI BUTZAKERS
 A VERY INTERESTING THING WE ARE NOT GOING TO USE THE FULL GRADIENT WE ARE ONLY GOING TO USE THE GRADIENTS OF INDIVIDUAL COMPONENTS RIGHT SO WHAT DOES THE MINIMUM OF AN INDIVIDUAL COMPONENT LOOK LIKE WELL THE MINI
 OF AN INDIVIDUAL COMPONENT IS ATTAINED WHEN WE CAN SET THIS THING TO ZERO AND THAT THING BECOMES ZERO IF YOU JUST PACK X EQUAL TO B I DIVIDED BY A RIGHT SO A SINGLE COMPONENT WE CAN BE MINIMIZED BY
 THAT CHOICE SO YOU CAN DO A LITTLE BIT OF ARITHMETIC MEAN GEOMETRIC MEAN TYPE INEQUALITIES TO DRAW THIS PICTURE
 SO OVER ALL I FROM ONE THROUGH N THIS IS THE MINIMUM VALUE OF THIS RATIO A I BY B I AND LET'S SAY THIS IS THE MAX VALUE
 OF A I B B I AND WE KNOW THAT CLOSE FORMED SOLUTION THAT IS THE TRUE SOLUTION SO YOU CAN VERIFY WITH SOME ALGEBRA THAT THAT SOLUTION WILL LIE
 IN THIS INTERVAL SO LET'S SO YOU MAY WANT TO THIS IS A TINY EXERCISE FOR YOU HOPEFULLY SOME OF YOUR LOVE INEQUALITIES LIKE ME SO THIS IS HOPEFULLY
 NOT SUCH A BAD EXERCISE BUT YOU CAN VERIFY THAT WITHIN THIS RANGE OF THE INDIVIDUAL MINNS AND MAX IS WHERE THE COMBINED SOLUTION LIES SO OF COURSE INTUITIVELY YOU WOULD HAVE PHYSIC STYLE THINKING YOU WOULD HAVE GUESSED THAT RIGHT AWAY
 SO MEANS WHEN YOU'RE OUTSIDE WE'RE THE INDIVIDUAL SOLUTIONS LET'S CALL THIS THE FAR OUT ZONE AND ALSO THIS SIDE IS THE FAR OUT ZONE AND THIS REGION
 WITHIN WHICH THE TRUE MINIMUM CAN LIE YOU CAN SAY OERAS THE REGION OF CONFUSION WHY I'M CALLED CALLING IT THE REGION OF CONFUSION BECAUSE THERE BY MINIMIZING AN INDIVIDUAL F I YOU'RE NOT GOIN
 TO BE ABLE TO TELL WHAT IS THE COMBINED X STAR THAT'S ALL AND A VERY INTERESTING THING HAPPENS NOW JUST TO GAIN SOME MATHEMATICAL INSIGHT INTO THAT SIMULATION THAT I SHOWED YOU THAT IF YOU HAVEA
 ORX THAT IS OUTSIDE THIS REGION OF CONFUSION WHICH STATES THAT IF YOU'RE FAR FROM THE REGION WITHIN WHICH AN OPTIMUM CAN LIE SO YOU'RE FAR AWAY YOU'VE JUST
 ED OUT YOUR PROGRESS YOU MADE A RANDOM INITIALIZATION MOST LIKELY YOU'RE FAR AWAY FROM WHERE THE SOLUTION IS O SUPPOSE THAT'S WHERE YOU ARE WHAT HAPPENS WHEN YOU'RE IN THAT FAR OUT REGION SO YOU'RE IN THAT FAR OUT REGION YOU USE A STOCCASTIC GRADE
 OF SOME ITH COMPONENT SO THE FULL GRADIENT WILL LOOK LIKE THAT A SARCASTIC GRADIENT LOOKS LIKE JUST ONE COMPONENT AND WHEN YOU'RE FAR OUT OUTSIDE THAT MIN AND MA
 REGIME THEN YOU CAN CHECK BY JUST BY JUST LOOKING AT IT THAT SARCASTIC GRADIENT IN THAT FAR AWAY REG
 HAS EXACTLY THE SAME SIGN AS THE FULL GRADIENT WHAT DOES GRADIENT DESCEND TO IT SAYS WELL WALK IN THE DIRECTION OF THE NEGATIVE GRADIENT AND IF FAR AWAY FROM THE OPTIMUM OUTSIDE THE REGION OF
 FUSION YOUR STOCASTIC GRADIENT HAS THE SAME SIGN AS THE TRUE GRADIENT MAYBE IN MORE LINARAL GEBRA TERMS IT MAKES YOU KNOW IT MAKES AN ACUTE ANGLE WITH YOUR GRADIENTS THAT MEANS IF EVEN THOUGH STOCAST
 GRADIENT IS NOT EXACTLY THE FULL GRADIENT IT HAS SOME COMPONENT IN THE DIRECTION OF THE TRUE GRADIENT THIS IS ONE DAY HERE IT IS EXACTLY THE SAME SIGN IN MULTIPLE DIMENSIONS THIS IS THE IDEA THAT IT WILL HAVE SOME COMPONENT IN THE
 DIRECTION OF THE TRUE GRADIENT WHEN YOU'RE FAR AWAY WHICH MEANS IF YOU THEN USE THAT DIRECTION TO MAKE AN UP DATE IN THAT STYLE YOU WILL END UP MAKING SOLID PROGRESS
 AND THE BEAUTY IS IN THE TIME IT WOULD HAVE TAKEN YOU TO DO ONE SINGLE ETRATION OF BACH GRADIAN DESCENT FAR AWAY YOU CAN DO MILLIONS TO CASTIC STEPS AND EACH STEP WILL MAKE SOME PROGRESS AND THAT'S WHY
 WE SEE THIS DRAMATIC IN ISHILL AGAIN THIS IS IN THE VANDY CASE THIS IS EXPLICIT MATHEMATICALLY IN THE HIDE CASE THIS IS MORE INTUITIVE WITHOUT FURTHER ASUMPTIONS ABOUT ANGLES ET CETERA ONE CAN'T MAKE SUCH A BROAD
 CLAIM BUT INTUITIVELY THIS IS WHAT'S HAPPENING ON WHY YOU SEE THIS AWESOME INITIAL SPEED AND ONCE YOU'RE INSIDE THE REGION OF CONFUSION THEN THIS BEHAVIOR
 BREAKS DOWN SOME SARCASTIC GRADIENT MAY HAVE THE SAME SIGN AS THE FULL GRADIENT SOME MAY NOT AND THAT'S WHY YOU CAN GET AT CRAZY FLUCTUATIONS SO THIS SIMPLE WONDAY EXAMPLE KIND OF EXACTLY SHOWS YOU WHAT WE SAW IN THAT
 PICTURE AND PEOPLE REALLY LOVE THIS INITIAL PROGRESS BECAUSE OFTEN WE ALSO DO EARLY STOPPING AND O TRAIN FOR SOME TIME AND THEN YOU SAY O K I'M DONE SO
 PORTANTLY IF YOU ARE PURELY AN OPTIMIZATION PERSON NOT THINKING SO MUCH IN TERMS OF MACHINE LEARNING THEN PLEASE KEEP IN MIND THAT STROCASTIC GRADIENT DESCENT O STROCASTIC GRADIENT METHOD IS NOT SUCH A GREAT
 ISATION METHOD BECAUSE ONCE IN THE REGION OF CONFUSION IT CAN JUST FLUCTUATE ALL OVER FOREVER AND IN MACHINE LUARNING YOU SAY OH THE REGION OF CONFUSION THAT'S FINE I I'LL MAKE MY METHOD ROBUST IT'LL MAKE MY NEURAL NETWORK TRAINING MORE ROBUST IT'LL GENERALIZE BE
 ET CETERA ET CETERA WE LIKE THAT SO IT DEPENDS ON WHICH ER FRAME OF MIND YOU'R EN OK SO THAT'S THAT'S THE AWESOME THING ABOUT THIS SOCASTIC GRADIENT METHOD SO
 I'LL GIVE YOU NOW THE KEY MATHEMATICAL IDEAS BEHIND THE SUCCESS OF S JD THIS WAS LIKE A LITTLE ILLUSTRATION VERY ABSTRACTLY THIS IS AN IDEA THAT RECORDS THROUGHOUT MACHINE
 LEARNING AND THROUGHOUT THEORETICAL COMPUTER SCIENCE AND STATISTICS ANY TIME YOU ARE FACED WITH THE NEED TO COMPUTE AN EXPENSIVE QUANTITY RESORT TO RANDOMIZATION TO SPEED UP THE COMPUTATION
 SGD IS ONE EXAMPLE THE TRUE GRADIENT WAS EXPENSIVE TO COMPUTE SO WE CREATE A RANDOMIZED ESTIMATE OF THE TRUE GRADIENT AND THE RANDOMIZ ESTIMATE IS MUCH FASTER TO
 AND MATHEMATICALLY WHAT WILL START HAPPENING IS DEPENDING ON HOW GOOD YOUR RANDOMIZED ESTIMATE IS YOUR METHOD MAY OR MAY NOT CONVERGE TO THE RIGHT ANSWER
 SO OF COURSE ONE HAS TO BE CAREFUL ABOUT WHAT PARTICULAR RANDOMIZED ESTIMATE ONE MAKES SO BUT REALLY ABSTRACTLY EVEN IF I HADN'T SHOWN YOU THE MAIN IDEA THIS IDEA YOU CAN APPLY IN MANY OTHER SETTINGS IF YO HEVER
 DIFFICULT QUANTITY COME OF IT THE RANDOM IS ESTIMATE AND SAVE ON COMPUTATION THIS IS A VERY IMPORTANT THEME THROUGHOUT MACHINE LEARNING AND LATER SCIENCE OK AND THIS IS THE KEY PROPERTY SO
 STOCASTIC GRADIENT DESCENT IT USES STOCASTIC GRADIENTS STOCASTIC IS HERE JUST USED VERY LOOSELYAN IT JUST MEANS THERE'S SOME RANDOMIZATION THAT'S ALL IT MEANS AND THE PROPERTY THE KEY PROPERTY THAT WE
 HAVE IS IN EXPECTATION THE EXPECTATION IS OVER WHATEVER RANDOM NESS YOU USED SO IF YOU PICKED YOU KNOW SOME RANDOM TRAINING DATA POINT OUT OF THE MILLION
 THEN IT IS THE EXPECTATION IS OVER THE PROBABILITY DISTRIBUTION OVER WHAT KIND OF RANDOM NESS YOU USED IF YOU LIKE PICKED UNIFORMLY AT RANDOM FROM A MILLION POINTS THEN THIS EXPECTATION IS OVER THAT UNIFORM PROBABILITY BUT THE KEY PROPERTY FOR
 G D OR AT LEAST THE VERSION OF S G D I'M TALKING ABOUT IS THAT IN EXPECTATION OVER THAT RANDOM NESS THE THING THAT YOU'RE PRETENDING TO USE INSTEAD OF THE TRUE GRADIENT IN EXPECTATION ACTUALLY IT IS THE TRUE GRAD
 SO IN STATISTICS LANGUAGE THIS IS CALLED THE STOCASTIC GRADIENT THAT WE USE IS AN UNBIASED ESTIMATE OF THE TRUE GRADIENT AND THIS IS A VERY IMPORTANT PROPERTY IN THE MAT
 METICAL ANALYSIS OF SOCASTIC RADIANT DESCENT THAT IT IS AN UNBIASED ESTIMATE AND INTUITIVELY SPEAKING ANY TIME YOU DID ANY PROOF IN CLASS OR IN THE BOOK OR LECTURE NOTES WHEREVER WHERE YOU WERE USING
 TRUE GRADIENTS MORE OR LESS YOU CAN DO THOSE SAME PROOFS MORE OR LESS NOT ALWAYS USING STARCASTIC GRADIENTS BY REPLACE BY AND CAPTULATING EVERYTHING WITHIN EXPECTATIONS OVER THE RANDOMNESS
 I'LL SHOW YOU AN EXAMPLE OF WHAT I MEAN BY THAT I'M JUST TRYING TO SIMPLIFY THAT FOR YOU AND IN PARTICULAR THE UNBIASED NESS IS GREAT SO IT MEANS I CAN KIND OF PLUG
 IN THESE STARCASTIC GRADIENTS IN PLACE OF THE TRUE GRADIENT AND I'M STILL DOING SOMETHING MEANINGFUL SO THIS IS ANSWERING THAT EARLIER QUESTION YOU KNOW WHY THIS RANDOM STUFF WHY SHOULD WE THINK IT MAY WORK BUT THERE'S ANOTHER
 IMPORTANT ASPECT TO IT WHY IT WORKS BEYOND THISER UNBIASED NESS THAT THE AMOUNT OF NOISE OR LIKE THE AMOUNT OF ER THE AMOUNT OF STOCASTICITY IS CONTROLLED SO JUST BECAUSE
 IS AN UNBIASED ESTIMATE DOESN'T MEAN THAT IT'S GOING TO WORK THAT WELL WHY BECAUSE IT COULD STILL FLUCTUATE HUGELY RIGHT ESSENTIALLY PLUS INFINITY HERE MINUS INFINITY HERE YOU TAKE AN AVERAGE YOU GET ZERO
 SO THAT IS ESSENTIALLY UNBIASSED BUT THE FLUCTUATION IS GIGANTIC SO WHENEVER TALKING ABOUT ESTIMATES WHAT'S THE OTHER KEY QUANTITY WE NEED TO CARE ABOUT BEYOND EXPECTATION XPERIECE
 AND REALLY THE KEY HAL THAT GOVERNS THE SPEED AT WHICH SOCASTIC GRADIENT DESCENT DOES THE JOB THAT WE WANTED TO DO IS HOW MUCH VARIANCE DO THE STOCASTIC GRADIENTS HAVE
 JUST THIS SIMPLE STATISTICAL POINT IN FACT IS AT THE HEART OF A SEQUENCE OF RESEARCH PROGRESS IN THE PAST FIVE YEARS IN THE FIELD OF STOCASTIC GRADIENT WHERE PEOPLE HAVE WORKED REALLY HARD TO COME UP WITH NEW
 AND NEWER FANCIER AND FANCIER VERSIONS OF STOCASTIC GRADIENT WHICH HAVE THE UNBIASENESS PROPERTY BUT HAVE SMALLER AND SMALLER VARIENTS AND THE SMALLER THE VARIANTS YOU HAVE THE BETTER YOUR STOCASTIC GRADIENT
 IS AS A REPLACEMENT OF THE TRUE GRADIENT AND OF COURSE THE BETTER IS IS THE REPLACEMENT OF THE TRUE GRADIENT THEN YOU TRULY GET THAT IN N TIME SPEED UP
 SO THE SPEED OF CONVERGENCE DEPENDS ON HOW NOISY THE SARCASTIC GRADIENTS ARE IT SEEMS LIKE I'M GOING TOO SLOW I WON'T BE ABLE TO DO A PROOF I WITH SUCKS BUT I
",14471,2668
