THE FOLLOWING CONTENT IS PROVIDED UNDER A CREATIVE COMMONS LICENSE YOUR SUPPORT WILL HELP M I T OPEN COURSE WARE CONTINUE TO OFFER HIGH QUALITY EDUCATIONAL RESOURCES FOR FREE TO MAKE A DONATION OR TO VIEW ADDITIONAL MATERIALS FROM HUNDREDS OF M
 COURSES VISIT M I T OPEN COURSE WARE AT O C W DOT M I T DOT E D UNES PROFESSOR SRAT TO WITHDRAW FROM MY WHOA TAGT SIX O THREE THINGS
 AND THE GRADUATE VERSION AND MAYBE SOME OF YOU HADINI ONE OR OTHER OF THOSE CLASS SO HE D GRACIOUSLY AGREE TO COME TO DAY AND ANE TO TALK ABOUT THE CAT
 PREVIOUS ASCENT ST B AND IT SO OR WE WERE NOT QUIE ONE O FIVE BUT CLOSE AS I
 EVERYTHING IS READY THEN OK OD AND AND YOUR CUT OFF IS LIKE ONE FIFTY FIVE YE OK BUT IT'S NOT A SHARP CUT OFF
 WHY IS THE FLUCTUATION AE SUDDENLY CHANGE THEIR RESOLUTIONSBUT THAT'S FINE DOESNT DOESN'T BOTHER US SO
 I'M GOING TO TELL YOU ABOUT LET'S SAY ONE OF THE MOST ANCIENT OPTIMIZATION METHODS MUCH SIMPLER THAN IN FACT THE MORE ADVANCED METHODS YOU HAVE ALREADY SEEN IN CLASS AND INTERESTINGLY
 MORE ANCIENT METHOD REMAINS THE METHOD FOR TRAINING LARGE SCALE MACHINE LEARNING SYSTEMS AND SO THERE'S A LITTLE BIT OF HISTORY AROUND THAT I'M NOT GOING TO GO TOO MUCH INTO THE HIST
 BUT THE BOTTOM LINE WHICH PROBABLY ER GILL HAS ALSO MENTIONED TO YOU IN CLASS THAT AT LEAST FOUR LARGE DATA SCIENCE PROBLEMS IN THE END STUFF REDUCES TO SOLVING AN OPTIMIST
 TION PROBLEM AND IN CURRENT TIMES THESE OPTIMIZATION PROBLEMS ARE PRETTY LARGE SO PEOPLE ACTUALLY STARTED LIKING STUFF LIKE GRADIENT DESCENT WHICH WAS INVENTED BY COSHE BACK IN THE DAY
 AND THIS IS HOW I AM WRITING THE ABSTRACT PROBLEM AND WHAT IER WANT TO SEE IS IS IT FITTING ON THE PAGE THIS IS MY IMPLEMENTATION IN MATLAB OF GRADIANT DESCENT JUST TO SET THE STAGE THAT
 THIS STUFF REALLY LOOKS SIMPLE YOU'VE ALREADY SEEN GRADIENT DESCENT AND TODAY ESSENTIALLY IN A NUTSHELL WHAT REALLY CHANGES IN THIS IMPLEMENTATION OF GRADIENT DESCENT TO DAY IS THIS PART
 THAT'S IT SO YOU'VE SEEN GRADIENT DESCENT I'M ONLY GOING TO CHANGE THIS ONE LINE AND THE CHANGE OF THAT ONE LINE SURPRISINGLY IS DRIVING YOU KNOW ALL THE DEEP LEARNING TOOL BOXES AND ALL OF LORD
 MACHINE LEARNING ET CETERA THIS IS AN OVER SIMPLIFICATION BUT MORALLY THAT'S IT SO LET'S LOOK AT WHAT'S HAPPENING SO I WILL BECOME E VERY CONCRETE PRETTY SOON BUT ABSTRACTLY
 I WANT YOU TO LOOK AT IS THE KINDS OF OPTIMIZATION PROBLEMS WE ARE SOLVING IN MACHINE LEARNING AND I'LL GIVE YOU VERY CONCRETE EXAMPLES OF THESE OPTIMIZATION PROBLEMS SO THAT YOU CAN RELATE TO TE
 BERRER BUT I'M JUST WRITING THIS AS THE KEY TOPIC THAT ALL THE OPTIMIZATION PROBLEMS THAT I'M GOING TO TALK ABOUT TO DAY THEY LOOK LIKE THAT YOU'RE TRYING TO FIND AN X OVER A COST
 UNCTION WHERE THE COST FUNCTION CAN BE WRITTEN AS A SUM IN MODERN DAY MACHINE LEARNING PARLANC THESE ARE ALSO CALLED FINITE SUM PROBLEMS IN CASE YOU RUN INTO THAT TERM AND WHIH UST CALL FINITE BECAUSE N IS FINITE HERE
 IN PURE OPTIMIZATION THEORY PER LANCE N CAN ACTUALLY GO TO INFINITY AND THEN THEY ARE CALLED STOCASTIC OPTIMIZATION PROBLEMS JUST FOR TERMINOLOGY IF WHILE SEARCHING THE INTERNET YOU RUN INTO SOME SUCH TERMINOLOGY SO YOU CAN
 KNOW WHAT IT MEANS SO HERE IS OUR SET UP IN MACHINE LEARNING WE HAVE BUNCH OF TRAINING DATA ON THIS SIDE I
 I'M CALLING X ONE THROUGH X N THESE ARE THE TRAINING DATA THE RAW FEATURES LATER ACTUALLY I'LL STOP WRITING X FOR THEM I'LL WRITE THEM WITH THE LETTER A BUT HOPPULLY THAT'S O K SO X ONE THROUGH X N THESE COULD BE
 RAW IMAGES FOR INSTANCE IN IMAGINETE OR SOME OTHER IMAGE DATA SET THERE COULD BE TEXT DOCUMENTS THEY COULD BE ANYTHING WY ON THROUGH Y N IN CLASSICAL MACHINE LEARNING THINK OF THEM AS PLUS MINUS ONE LABELS
 NOT CAT OR IN A REGRESSION SET UP AS SOME REAL NUMBER SO THAT'S OUR TRAINING DATA WE HAVE D DIMENSIONAL RAW VACTORS AN OF THOSE AND WE HAVE CORRESPONDING LABELS WHICH CAN BE EITHER PLUS
 MINUS ONE IN A CLASSIFICATION SETTING OR A REAL LUMBER IN A REGRESSION SETTING IT'S KIND OF IMMATERIAL FOR MY LECTURE RIGHT NOW SO THAT'S THE INPUT AND WHENEVER ANYBODY SAYS LARGE SCALE MACHINE LEARNING
 WHAT DO WE REALLY MEAN WHAT WE MEAN IS THAT BOTH N AND D CAN BE LARGE SO WHAT DOES THAT MEAN IN WORDS THE N IS THE NUMBER OF TRAINING DATA POINTS SO N COULD BE THESE DAYS
 WHAT MILLION TEN MILLION HUNDRED MILLION DEPENDS ON HOW BIG COMPUTERS AND DATA SETS YOU GOT SO N CAN BE HUGE D THE DIMENSIONALITY THE VECTORS THAT WE ARE WORKING WITH THE RAW VECTORS THAT CAN ALSO BE PRETTY LARGE THINK IF X
 IS AN IMAGE IF IT'S A MEGGA PIXEL IMAGE WHO D'S LIKE A MILLION ALREADY IF YOU'RE SOMEBODY LIKE CRATIO OR FACE BOOK OR GOGLE AND YOUR SERVING WEB ADVERTISEMENTS D THESE ARE THE
 URS COULD BE LIKE IN SEVERAL HUNDRED MILLION EVEN A BILLION WHERE THEY ENCLUDE ALL SORTS OF NASTY STUFF AND INFORMATION THEY COLLECT ABOUT YOU AS USERS SO SO MANY NASTY THINGS THEY CAN COLLECT RIGHT SO D AND ENA
 HUGE AND IT'S BECAUSE BOTH D AND N ARE HUGE WE ARE INTERESTED IN THINKING OF OPTIMIZATION METHODS FOR LARGE SKILLE MACHINE LEARNING THAT CAN HANDLE SUCH BIG D AND N AND THIS IS DRIVE
 A LOT OF RESEARCH ALSO IN THEORETICAL COMPUTER SCIENCE INCLUDING THE SEARCH FOR SUB LINEAR TIME ALGORITMS AND ALL SORTS OF DATA STRUCTURES AND HASHING TRICKS JUST TO DEAL WITH THESE TWO QUANTITIES SO HERE IS AN EXAMPLE SUPER TOY EXAM
 AND I HOPE REALLY YOU KNOW THAT I CAN SQUEEZE IN A LITTLE BIT OF PROOF LATER ON TOWARDS THE END I'LL TAKE A VOTE HERE IN CLASS TO SEE IF YOU'RE INTERESTED LET'S LOOK AT THE MOST CLASSIC QUESTION LEAST SQUARES REGRESSION
 AS MATRIX OF OBSERVATIONS OR SORRY MEASUREMENTS B ARE THE OBSERVATIONS YOU'RE TRYING TO SOLVE A X MINUS B WHOLE SQUARE OF COURSE A LINEAR SYSTEM OF EQUATIONS THE MOST CLASSICAL PROBLEM IN LINEAR ALGEBRA CAN ALSO BE WRITTEN LIKE THAT LET'S SAY
 THIS CAN BE EXPANDED HOPEFULLY YOU ARE COMFORTABLE WITH THIS NORM RIT SO X TWO SQUARE THIS IS JUST DEFINED AS
 THAT'S A DEFINITION OF THAT NOTATION BUT I'LL JUST WRITE IT ONLY ONCE NOW I HOPE YOU ARE FULLY FAMILIAR WITH THAT SO BY EXPANDING THAT I MANAGED TO WRITE LEE SQUARE'S PROBLEM IN TERMS OF
 WHAT I CALL THE FINITE SUM RIGHT TIS JUST GOING OVER ALL THE ROWS IN A THEIR END ROWS LET'S SAY SO THAT'S THE LEAST SQUARE CLASSICAL L SQUARES PROBLEM IT ASSUMES THIS FINITE SUM FORM THAT WE CARE ABOUT
 ANOTHER RANDOM EXAMPLE SOMETHING CALLED LASSO MAYBE YOU HAVE IF ANYBODY OF YOU HAS PLAYED WITH MACHINE LEARNING OR STATISTIC TOOLKITS YOU MAY HAVE SEEN SOMETHING CALL LASSO LASSO IS ESSENTIALLY LEAST SQUARES BUT THERE'S ANOTHER
 SIMPLE TERM AT THE END THAT AGAIN LOOKS LIKE F OF I SUPPORT VECTOR MACHINES ONCE A WORK HORSE OF THEY 'RE STILL A WORKHORSE HORSE OF PEOPLE WHO WORK WITH SMALL TO MEDIUM SIZED
 DATA DEEP LEARNING STUFF REQUIRES HUGE AMOUNT OF DATA IF YOU HAVE SMALL TO MEDIUM AMOUNT OF DATA LOGISTIC REGRESSION SUPPORT VECTOR MACHINES TREES ET CETERA THESE WILL BE YOUR FIRST GO TO METHODS THEY ARE STILL VERY WIDELY USED THESE PROBLEMS ARE AGAIN
 WRITTEN IN TERMS OF A LOSS OVER TRAINING DATA SO THIS AGAIN HAS THIS AWESOME FORMET WHICH I JUST NOW RECORD HERE I MAY NOT EVEN NEED TO REPEAT IT SOMETIMES I WRITE IT WITH A NORMALIZATION
 YOU MAY WONDER AT SOME POINT WHY AS THAT FINITE SUM PROBLEM AND MAYBE THE EXAMPLE THAT YOU WANTED TO SEE IS SOMETHING LIKE THAT
 SO DEEP NEURAL NETWORKS THAT ARE VERY POPULAR THESE DAYS THEY'RE JUST YET ANOTHER EXAMPLE OF THIS FINITE SUM PROBLEM HOW ARE THEY AN EXAMPLE OF THAT SO YOU HAVE TEN TRAINING DATA POINTS THERE'S
 URAL NETWORK LAWS LIKE CROSS ENTROPY OR WHAT HAVE YOU SQUARED LAWS CROSS ANT ANY KIND OF LAWS WY EYES ARE THE LABELS CAT NOT CAT OR WHATEVER OF THE MAYBE A MULTI CLASS AND THEN YOU HAVE A TRANSFER
 FUNCTION CALLED THE DEEP NEURAL NETWORK WHICH TAKES RAW IMAGES AS IMPORT AND GENERATES A PREDICTION WHETHER THIS IS A DOG OR NOT THAT WHOLE THING I'M JUST CALLING D AND N IS A FUNCTION OF AIS WHICH ARE THE TRAINING DATA X
 ARE THE WEIGHT MATRECES OF THE NEURAL NETWORK SO I'M JUST COMPRESSED THE WHOLE NEURAL NETWORK INTO THIS NOTATION ONCE AGAIN IT'S NOTHING BUT AN INSTANCE OF THAT FINITE SUM SO THAT F I IN THERE CAPTURES THE ENTIRE NEURAL
 WORK ARCHITECTURE BUT MATHEMATICALLY IS STILL JUST ONE PARTICULAR INSTANCE OF THIS FINITE SUM PROBLEM AND IN PEOPLE WHO DO A LOT OF STATISTICS MAXIMUM LIKELIHOOD ESTIMATION
 THIS IS LOG LIKELIHOOD OVER AND OBSERVATIONS WE WANT TO MAXIMIZE LOG LIKELIHOOD ONCE AGAIN JUST A FINITE SUM SO PRETTY MUCH MOST OF THE PROBLEMS THAT WE'RE INTERESTED IN
 MACHINE LEARNING AND STATISTICS WHEN I WRITE THEM WRITE THEM DOWN AS AN OPTIMIZATION PROBLEM THEY LOOK LIKE THESE FINITE SUM PROBLEMS AND THAT'S THE REASON TO DEVELOP SPECIALIZED OPTIMIZATION PROCEDURES TO SOLVE UC
 FINITE SOME PROBLEMS AND THAT'S WHERE S JIDDY COMES IN O KESS TO THAT'S KIND OF JUST THE BACK DROP LET'S LOOK AT NOW HOW TO GO ABOUT SOLVING THESE PROBLEMS
 SO HOPEFULLY THIS ITERATION IS FAMILIAR TO YOU GRADIENT DESCENT RIGHT O SO JUST FOR NOTATION
 F OF X REFERS TO THAT ENTIRE SUMMATION F SUB I OF X REFERS TO A SINGLE COMPONENT O SO IF YOU WERE TO TRY TO SOLVE THAT IS TO MINIMIZE THIS COST FUNCTION NEURA NETWORK ESPEM
 HAVE YOU USING GRADIENT DESCENT THAT'S WHAT ONE ITERATION WOULD LOOK LIKE BECAUSE IT'S A FINITE SUM GRADIENTS ARE LINEAR OPERATORS YOU KNOW GRADIENT OF THE SUM IS THE SUM OF THE GRADIENTS SO THAT'S
 AN DESCENT FOR YOU AND NOW I'LL JUST ASK A RHETORIC QUESTION THAT IF YOU PUT YOURSELF IN THE SHOES OF YOURNO ELGRETHAM DESIGNERS SOME THINGS THAT YOU MAY WANT TO THINK ABOUT WHAT MAY YOU NOT
 ABOUT THIS ITERATION GIVEN THAT BIG AND BIG D STORY THAT I TOLD YOU SO ANYBODY HAVE ANY RESERVATIONS OR ABOUT DRAWBACKS OF THIS ITERATION ANY COMMENTS REALLY SO
 IT'S A PRETTY BIG SUM SPECIALLY NS MILLION OR SOME BIGGER BAZILIAN SANOER THAT IS DEFINITELY A BIG DRAWBACK AND THERE BTHAT IS THE PRIME DRAWBACK FOR LARGE SKAL
 THE END CAN BE HUGE THERE CAN BE A VARIETY OF OTHER DRAWBACKS SOME OF THOSE YOU MAY HAVE SEEN PREVIOUSLY WHEN PEOPLE COMPARE WHETHER TO DO GRADIENT OR TO DO NEWTON ET CETERA BUT FOR PURPOSE OF TO DAY FOR FINITE SUMS THE BIG DRAWBACK
 COMPUTING GRADIENT AT A SINGLE POINT TIS SUBSCRIPT EXCAMISING THERE INVOLVES COMPUTING THE GRADIENT OF THAT ENTIRE SUM THAT SUM IS HUGE SO GETTING A SINGLE GRADIENT TO DO
 SINGLE STEP OF GRADIENT DESCENT FOR A LARGE DATA SET COULD TAKE YOU HOURS OR DAYS SO THAT'S A MAJOR DRAWBACK BUT THEN OK IF YOU IDENTIFY THAT DRAWBACK ANYBODY HAVE
 ANY IDEAS HOW TO COUNTER THAT DRAWBACK AT LEAST SAY PURELY FROM AN ENGINEERING PERSPECTIVE I HEARD SOMETHING AN HE
 USING LIKE SOME KIND OF ATCH YOU'RE WELL AHEAD OF MY SLIDES WE ARE COMING TO THAT AN MAYBE SOMEBODY ELSE HAS ESSENTIALLY THE SAME I ANYBODY WANTS TO SUGGEST HOW TO CIRCUMVENT THAT
 AND STUFF IN THERE ANYTHING SUPPOSE YOURE AGREEMENTIN THATS WHAT WOULD YOU DO AT EE EY ONE EXAMPLE AT O TINE SO MYSE SO
 RANDOM SAMPLE OF THE FULL N SO THESE ARE ALL EXCELLENT IDEAS AND HENCE YOU FOLKS IN THE CLASS HAVE DISCOVERED THE MOST IMPORTANT METHOD FOR OPTIMIZING MACHINE LEARNING PROBLEMS
 ING HERE IN A FEW MOMENTS ISN'T THAT GREAT SO THE PART THAT IS MISSING IS OF COURSE TO MAKE SENSE OF DOES THIS IDEA WORK WHY DOES IT WORK SO THIS IDEA ER IS REALLY AT THE HEART OF A SARCASTIC RADIANT DESCENT
 SO LET'S SEE MAYBE I CAN SHOW YOU AN EXAMPLE ACTUALLY THAT I ER I'LL SHOW YOU A SIMULATION I FOUND ON SOMEBODY'S A NICE
 ABOUT THAT SO EXACTLY YOUR IDEA JUST PUT IN SLIGHT MATHEMATICAL NOTATION THAT WHAT IF AT EACH ITURATION WE RANDOMLY PICK SOME INTEGER I K
 OUT OF THE N TRAINING DATA POINTS AND AND WE INSTEAD JUST PERFORM THIS UP DATE RIGHT SO YOU INSTEAD OF USING THE FULL GRADIENT
 YOU JUST COMPUTE THE GRADIENT OF A SINGLE RANDOMLY CHOSEN DATA POINT WHAT HAVE YOU DONE WITH THAT ONE ITRATION IS NOW EN TIMES FASTER IF N WERE A MILLION OR A BILLION WHOW THAT'S
 SUPER FAST BUT WHY SHOULD THIS WORK RIGHT I MEAN I COULD HAVE DONE MANY OTHER THINGS I COULD HAVE CONSTAD I COULD HAVE NOT DONE ANY UP DATE AND JUST OUTPUT THE ZERO ACTOR THAT WOULD TAKE EVEN LESSER TIME
 THAT'S ALSO AN IDEA I'S A BAD IDEA BUT IT'S AN IDEA IN THE SIMILAR LEAGUE I COULD HAVE DONE A VARIETY OF OTHER THINGS WHY WOULD YOU THINK THAT JUST IN REPLACING THAT SUM WITH JUST ONE RANDOM EXAMPLE MAY WORK LET'S SEE A
 BIT MORE ABOUT THAT SO OF COURSE IT'S EN TIMES FASTER AND THE KEY QUESTION FOR US HERE RIGHT NOW THE SCIENTIFIC QUESTION IS DOES THIS MAKE SENSE
 IT MAKES GREAT ENGINEERING SENSE DOES IT MAKE ALRHYTHMIC OR MATHEMATICAL SENSE SO THIS IDEA OF DOING STUFF IN THIS STOCASTIC MANNER WAS ACTUALLY ORIGINALLY PROPOSED BY ROBINS AND MONROE SOMEWHERE I THINK
 NINETEEN FIFTY ONE AND THAT'S THE MOST ADVANCED METHOD THAT WE'RE ESSENTIALLY USING CURRENTLY SO I'LL SHOW YOU THAT THIS IDEA MAKES SENSE BUT MAYBE LET'S FIRST JUST LOOK AT A COMPARISON OF AS
 DY WITH GRADIENT DESCENT IN THIKI SIMULATION SO THIS IS THAT MATLAP COD OF GRADIENT DESCENT AND
 THIS IS JUST A SIMULATION OF GRADIENT DESCENT AS YOU PICK A DIFFERENT STEP SIZE THAT GAMMA IN THERE YOU MOVE TOWARDS THE OPTIMUM IF THE STEP SIZE IS SMALL YOU MAKE MANY SMALL STEPS AND YOU GRADUALLY KEEP MAKING SLOW
 PROGRESS ANEW REACH THERE THAT'S FOR A WELL CONDITIONED PROBLEM IN AN ILL CONDITIONED PROBLEM IT TAKES YOU EVEN LARGER IN A NEURAL NETWORK TYPE PROBLEM WHICH IS NON CONVEX YOU HAVE TO TYPICALLY WORK WITH SMALLER STEP SIZES AND IF YOU TAKE BIGGER
 ONCE YOU CAN GET CRAZY OSCILLATIONS BUT THAT'S GRADIEN DESCENT IN COMPARISON LET'S HOPE THAT THIS LOADS CORRECTLY OR THERE'S EVEN A PICTURE OF ROBINS WHO WAS A COL DISCOVERER
 THE SOCASTIC GRADIENT METHOD THERE'S A NICE SIMULATION THAT INSTEAD OF MAKING THAT KIND OF DETERMINISTIC DESCENT AFTER ALL GRADIENT DESCENT IS CALLED GRADIENT DESCENT AT EVERY STEP
 IT DESCENDS IT DECREASES THE COST FUNCTION STOCASTIC GRADIENT DESCENT IS ACTUALLY A MIS NOMER AT EVERY STEP IT DOESN'T DO ANY DESCENT IT DOES NOT DECREASE THE COUSE FUNCTIONS WE CAN SEE AT EVERY STEP THOSE ARE THE CONTOURS OF THE
 FUNCTION SOMETIMES IT GOES UP SOMETIMES IT GOES DOWN IT FLUCTUATES AROUND BUT IT KIND OF SORCASTICALLY STILL SEEMS TO BE MAKING PROGRESS TOWARDS THE OPTIMA AND STOCASTIC GRADIENT DESCENT BECAUSE IT'S NOT USING EX
 AT GRADIENTS JUST WORKING WITH THES RANDOM EXAMPLES IT ACTUALLY IS MUCH MORE SENSITIVE TO STEP SIZES AND YOU CAN SEE AS I INCREASE THE STEP SIZE ITS BEHAVIOR THIS IS LIKE ACTUALLY
 SIMULATION FOR A TOY PROBLEM SO INITIALLY SO WHAT I WANT YOU TO NOTICE IS LET ME GO THROUGH THIS ER YOU KNOW A FEW TIMES KEEP LOOKING AT WHAT PATTERNS YOU MAY NOTICE IN HOW
 LINE IS FLUCTUATING HOPEFULLY THIS IS BIG ENOUGH FOR EVERYBODY TO SEE OK SO THIS SLIDER THAT I'M SHIFTING IS JUST THE STEP SIZE SO LET ME JUST REMIND YOU IN CASE YOU FORGOT THE ITERATION WE ARE RUNNING ESCAPEES
 ONE IS X K MINUS SOM TAK ITS CALLED ALPA TER
