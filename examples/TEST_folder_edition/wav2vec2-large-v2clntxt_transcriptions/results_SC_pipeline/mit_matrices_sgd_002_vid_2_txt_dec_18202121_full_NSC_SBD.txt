Well let's let me actually tell you then about rather than the the proof I think I'll share the proof with kill because the proof that I want wanted to actually show you gives a proof of. The stocastic gradient is well behaved on both convex and non convex problems and the proof it wanted to show was for the non convex case because it applies to neural network so you may be curious about that proof and remarkably that proof is much simpler than the case of convex. Problems so let me just mention some very important points about sarcastic radiant so even though this method has been around since nineteen fifty one every deep learning toolkit has it and we are studying it in class there are still gapsbet. In what we can say theoretically and what happens in practice and I'll show you those gaps already and encourage you to think about those if you wish so let's look back at our problem and tell you about two variants so here are the two variants I'm going. Ask if any of you is familiar with these variants in some way or other so let's is just called it feasible here there are no constraints so you start with any random factor of your choice in deep network train. You have to work harder and then this is the iteration you run right option one and option to so option one says that was the idea we had in class random by pick some training data point use its futuristic gradient. Well what do we mean by random by pick the moment you use the word random you have to define what's the random news so the random news is uniform probability. And training data points that is one's randomness the other version is you pick a training data point without replacement so so with replacement means you know just uniformly at random each time. Draw a number from one through a use that sarcastic radiant move on which means the same point can easily be picked twice also and without replacement means if you've picked up point number three you're not going to pick it again until you've. Through the entire training data set those are two types of randomness which version would you use there is no right or wrong answer to this I'm just taking a pole. What would you use think that you're writing a programme for this and maybe think really pragmatically practically that's enough of an in which version would you use I'm just curious. Who would use one please raise hands on who and the the exclusion the complement thereof or I don't know maybe some people are undecided who would use too very few people to. Of how many of you use the neural network training tool kits like dense flu pit torch what not which versions are they using. Actually every person in the real world is using versions too are you really going to randomly go through your ram each time to pick random points they'll kill your big top performance like anything. What people do is take a data set use a pre shuffle operation and then just whoop stream through the data what does streaming through the data mean without replacement so all the toolkits actually are using the without replacement or. Even though intuitively the re as uniform random feels much nicer and that feeling is not ill founded because that's the only version we know how to analyze mathematically so even for this method everybody studies it there. And papers on it the version that is used in practice is not the version we know how to analyze it's a major open problem in the field of stocastic gradient to actually analyze the version that we use in practice this kind of. Rising but it's without replacement means non big probability theory and non side probability theory is not so easy that's the answer to it so the other version is this main batch idea which. Mentioned really early on is that rather than pick one random point I'll pick a minnie batch so I had a million points each time instead of picking one maybe I'll pick. Ten or hundred or thousand or what have you so this averages things averaging things reduces the variants so this is actually a good thing because the more quantities you average the less noise you have that. Kind of what happens in probability right so we pick a minnie batch and the sarcastic estimate now is that you know not just a single gradient but averaged over a minnie batch. So mini batch of size one is the pure vanilla s s d mini batch of size and is nothing other than pure gradient descent something in between is what people actually use and again the theoretical analysis only adds. If the minnie batch is picked with replacement not without replacement is one of the reasons actually a very important thing in theory you don't gain too much in terms of computational gains on convergence. Speed by using many matches but many batches are really crucial especially in your deep learning your style training because they allow you to do things in parallel each thread or each core or sub. Or your small chip or what have you depending on your hardware can be working with one sorcastic radiant so mini batches the larger the minnie batch the more things you can do in parallel so mini batches are greatlyexplot. By people to air give you a cheap version of paralelism and where does the parallelism happen you can think that each core customer a sarcastic gradient so the hard part is not. Adding these things up and making the update to x the hard part is computing a sarcastic gradient so if you can compute ten thousand of those in parallel because you have ten thousand cores great for you and that's the reason people love using many. Aches but a nice side remark here this is also this brings us close to the research edge of things again that well you'd love to use very large minnie badges so that you can fully max out on the parade. Is available to you right maybe you have a multi gap system if younger friends with envy dear yoga i only have like two gps but depends on how many ups you have you'd like to really mix it on parallelism so that you can really. Inch through big data sets as fast as possible but you know what happens with very large many batches sifyo have very large mini batches starcastic gradient starts looking more like. Full gradient descent which is also called back gradient descentthat's not a bad thing that's awesome for optimization but it is a weird conundrum that happens in training deep metal networks. This type of problem we wouldn't have for convex optimization but in deep nuralet because this really disturbing thing happens that if you use these very large many batches your method starts resembling radiant dissent that means it decreases noise. Much so that this region of confusion shrinks so much which all sounds good but it ends up being really bad for machine learning that's what I said that in machine learning you want some region of uncertainty and what it. Means actually is a lot of people have been working on this including at big companies that if you reduce that region of uncertainty too much you end up over fitting your neural network. And then it starts sucking in its test data unseen data performance so even though for parallelism programming optimization theory big mini batch is awesome. Tunately their price to be paid that it hurts your test error performance and they're all sorts of methods people are trying to cook up including a shrinking data. Only or changing natural network architecture and all sorts of ideas you can cook up your ideas for your favourite architecture how to make a large minnie batch without hurting the final performance but it's still somewhat of an open question on how to optimal. Etc which means how large your many bats should be so even though these ideas are simple you see that every simple idea leads to an entire sub area of s g d. Here are practical challenges people have various heristics for solving these challenges you can cook up your own but it's not that one idea always works so if you look at s go do what are the. In parts the moving parts to go the gradients stacastic gradients the step size the mini batch so how should it pick step sizes very non trivial problem different deep learning to. S may have different ways of automating that tuning but it's one of the painful things which many batch to use with replacement without replacement is already showed you but which many batch should it use how large that should be again not. Easy question to answer how to computer starcastic gradients does anybody know how starcastic gradients are computed for deep network training anybody know there is. Famous elgarithm called back propagation that back propagation elgrithm is used to compute a single starcastic gradient some people use the word back prop to mean asjiddy but what back prop really means is some. Some kind of algorism which computer for you a single sarcastic gradient and hence youknow this are tenser filled central these toolkiss they come up with all sorts of waste automate the computation of a gradient because really that's the main thing. And then other ideas like radiant clipping and momentum and veteran the bunch of other ideas and the theoretical challenges is mentioned to you already proving that it works that it actually solves what it set out to do unfortunately it was too slow. Couldn't show you the awesome five line proof that I have that s s works for several networks and theoretical analysis as i said is really lagging my proof also uses the wind. Cement and the without replacement version which is the one that is actually implemented so there's very little progress on that there is some progress there's a bunch of papers including from our colleagues that insight but it's quite. And the biggest question which most of the people in machine learning are currently excited about these days is stuff like why does s go work so well for neural networks are. This scrappy optimization method it very rapidly does some fitting data is large neuro network is large and then this neuro network ends up having great classification performance why is that happening that's called trying to explain. Build a theory of generalization why does so s go the trained natural network work better than neural networks train with more fancy optimization methods it's a mystery and most of the people who take interest in theoretical machine learning and statistics. That is one of the mysteries they are trying to understand so I think that's my story of a jiddy and this is the part we shape but its take the the intuition behind us gods much more important than this. So I think we can thank you close somewhere the roof or mondays exactly I think so that. They'll be great