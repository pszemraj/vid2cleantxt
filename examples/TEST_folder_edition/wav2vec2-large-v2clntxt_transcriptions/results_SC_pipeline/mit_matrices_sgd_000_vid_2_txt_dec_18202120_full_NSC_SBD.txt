The following content is provided under a creative common license your support will help be on the open course we continue to offer high quality educational resources for free to make a donation or to view additional materials from hundreds of the. Courses visit up ensuring the open course was at o see w dot up i t dot e d unes professor start to withdraw from my what taught six to three things. And the graduate version and maybe some of you having one or other of those classes so he did graciously agree to come to day and have to talk about the chat. Previous ascent is public and it so far we were not quite one or five but close as it. Everything is ready then ok on and and your cut off is like one fifty five at ok but it's not a sharp cut off. Why is the fluctuation we suddenly change their resolutionsbut that's fine doesnt doesn't bother us so. I'm going to tell you about let's say one of the most ancient optimization methods much simpler than in fact the more advanced methods you have already seen in class and interestingly. More ancient method remains the method for training large scale machine learning systems and so there's a little bit of history around that I'm not going to go too much into the past. But the bottom line which probably her full has also mentioned to you in class that at least four large data science problems in the end stuff reduces to solving an optimist. Lion problem and in current times these optimization problems are pretty large so people actually started liking stuff like gradient descent which was invented by chose back in the day. And this is how I am writing the abstract problem and what they want to see is is it fitting on the page this is my implementation in matlab of gradiant dissent just to set the stage that. This stuff really looks simply you've already seen gradient descent and today essentially in a nutshell what really changes in this implementation of gradient descent to day is this part. That's it so you've seen gradient descent I'm only going to change this one line and the change of that one line surprisingly is driving you know all the deep learning tool boxes and all of food. Machine learning and veteran this is an over simplification but morally that's it so let's look at what's happening so it will become a very concrete pretty soon but abstractly. I want you to look at is the kinds of optimization problems we are solving in machine learning and I'll give you very concrete examples of these optimization problems so that you can relate to the. Better but I'm just writing this as the key topic that all the optimization problems that I'm going to talk about to say they look like that you're trying to find an x over a cost. Functions where the cost function can be written as a sum in modern day machine learning parlance these are also called finite some problems in case you run into that term and which must call finite because it is finite here. In pure optimization theory per lance n can actually go to infinity and then they are called stocastic optimization problems just for terminology if while searching the internet you run into some such terminology so you can. Know what it means so here is our set up in machine learning we have bunch of training data on this side and. I'm calling x one through x on these are the training data the raw features later actually I'll stop writing x for them I'll write them with the letter a but hopefully that's to a so x one through x on these could be. Raw images for instance in imaginete or some other image data set there could be text documents they could be anything way on through and and in classical machine learning think of them as plus minus the labels. Not cut or in a regression set up as some real number so that's our training data we have the dimensional raw factors as of those and we have corresponding labels which can be either plus. Minus one in a classification setting or a real lumber in a regression setting it's kind of immemorial for my lecture right now so that's the input and whenever anybody says large scale machine learning. What do we really mean what we mean is that both n and do can be large so what does that mean in words the end is the number of training data points so it could be these days. What millions ten million hundred million depends on how big computers and data sets you got so on can be huge in the dimensionality the protest that we are working with the raw receptors that can also be pretty large think if s. Is an image if it's a mega pixel image who d's like a million already if you're somebody like cratio or face book or google and your serving web advertisements do these are they. Us could be like in several hundred million even a billion where they include all sorts of nasty stuff and information they collect about you as users so so many nasty things they can collect right to do and even. Huge and it's because both do and n are huge we are interested in thinking of optimization methods for large skills machine learning that can handle such big do and on and this is drive. A lot of research also in theoretical computer science including the search for sub linear time algorithms and all sorts of data structures and hashing tricks just to deal with these two quantities so there is an example super toy exam. And I hope really you know that I can squeeze in a little bit of proof later on towards the end I'll take a vote here in class to see if you're interested let's look at the most classic question least squares regression. As matrix of observations or sorry measurements by are the observations you're trying to solve a x minus a whole square of course a linear system of equations the most classical problem in linear algebra can also be written like that let's say. This can be expanded hopefully you are comfortable with this norm it so x to square this is just defined as. That's a definition of that notation but I'll just write it only once now I hope you are fully familiar with that so by expanding that I managed to write the square's problem in terms of. What I call the finite some right is just going over all the rows in a there end rows let's say so that's the least square classical a squares problem it assumes this finite some form that we care about. Another random example something called lasso maybe you have if anybody of you has played with machine learning or statistic toolkits you may have seen something call lasso lasso is essentially least squares but there's another. Simple term at the end that again looks like that of a support victory machines once a work horse of they're still a workhorse horse of people who work with small to medium sized. Data deep learning stuff requires huge amounts of data if you have small to medium amount of data logistics regression support vector machines trees and veteran these will be your first go to methods they are still very widely used these problems are again. Written in terms of a loss over training data so this again has this awesome form which is just now record here i may not even need to repeat it sometimes and write it with a normalization. You may wonder at some point why as that finite some problem and maybe the example that you wanted to see is something like that. So deep neural networks that are very popular these days they're just yet another example of this finite some problem how are they an example of that so you have ten training data points there's. Rural network laws like cross entries or what have you squared laws cross and any kind of laws my eyes are the labels can not cut or whatever of the maybe a multi class and then you have a transfer. Function called the deep neural network which takes raw images as import and generates a prediction whether this is a dog or not that whole thing I'm just calling the and on is a function of his which are the training data x. Are the weight matreces of the neural network so I'm just compressed the whole neural network into this notation once again it's nothing but an instance of that finite sum so that if is in there captures the entire neural. Work architecture but mathematically is still just one particular instance of this finite some problem and in people who do a lot of statistics maximum likelihood estimation. This is long likelihood over and observations we want to maximize long likelihood once again just a finite sum so pretty much most of the problems that we're interested in. Machine learning and statistics when I write them write them down as an optimization problem they look like these finite some problems and that's the reason to develop specialized optimization procedures to solve such. Finite some problems and that's where a jiddy comes in a less to that's kind of just the back drop let's look at now how to go about solving these problems. So hopefully this iteration is familiar to your gradient descent right or so just for notation. If of x refers to that entire summation of sub i of x refers to a single component or so if you were to try to solve that is to minimize this cost function near network system. Have you using gradient descent that's what one iteration would look like because it's a finite some gradients are linear operators you know gradient of the sum is the sum of the gradients so that's. An dissent for you and now I'll just ask a rhetoric question that if you put yourself in the shoes of your elgretham designers some things that you may want to think about what may you not. About this iteration given that big and big the story that I told you so anybody have any reservations or about drawbacks of this iteration any comments really do. It's a pretty big sum especially as millions or some bigger bazilian sanoer that is definitely a big drawback and there that is the prime drawback for large scale. The end can be huge there can be a variety of other drawbacks some of those you may have seen previously when people compare whether to do gradient or to do newton et cetera but for purpose of the day for finite sums the big drawback. Computing gradient at a single point this subscribe examining there involves computing the gradient of that entire sum that sum is huge so getting a single gradient to do. Single step of gradient descent for a large data set could take you hours or days so that's a major drawback but then ok if you identify that drawback anybody have. Any ideas how to counter that drawback at least say purely from an engineering perspective I heard something as the. Using like some kind of match you're well ahead of my slides we are coming to that and maybe somebody else has essentially the same if anybody wants to suggest how to circumvent that. And stuff in there anything suppose your agreement that what would you do at me every one example at on time so issue so. Random samples of the full on so these are all excellent ideas and hence you folks in the class have discovered the most important method for optimizing machine learning problems. In here in a few moments isn't that great so the part that is missing is of course to make sense of does this idea work why does it work so this idea here is really at the heart of a sarcastic radiant descent. So let's see maybe I can show you an example actually that I fear I'll show you a simulation I found on somebody's a nice. About that so exactly your idea just put in slight mathematics notation that what if at each situation we randomly pick some integer in check. Out of the end training data points and and we instead just perform this up date right so you instead of using the full gradient. You just compute the gradient of a single randomly chosen data point what have you done with that one station is now ten times faster if you were a million or a billion show that's. Super fast but why should this work right I mean I could have done many other things I could have constant I could have not done any up date and just output the zero actor that would take even lesser time. That's also an idea it's a bad idea but it's an idea in the similar league I could have done a variety of other things why would you think that just in replacing that sum with just one random example may work let's see a. Bit more about that so of course it's ten times faster and the key question for us here right now the scientific question is does this make sense. It makes great engineering sense does it make alrhythmic or mathematical sense so this idea of doing stuff in this stocastic manner was actually originally proposed by robins and improve somewhere I think. Nineteen fifty one and that's the most advanced method that we're essentially using currently so I'll show you that this idea makes sense but maybe let's first just look at a comparison of as. Day with gradient descent in this simulation so this is that matlap code of gradient descent and. This is just a simulation of gradient descent as you pick a different step size that gamma in where you move towards the optimum if the step size is small you make many small steps and you gradually keep making slow. Progress anew reach there that's for a well conditioned problem in an ill conditioned problem it takes you even larger in a neural network type problem which is not convex you have to typically work with smaller step sizes and if you take bigger. Once you can get crazy oscillations but that's garden descent in comparison let's hope that this loads correctly or there's even a picture of robins who was a cool discoverer. The socastic gradient method there's a nice simulation that instead of making that kind of deterministic descent after all gradient descent is called gradient descent at every step. It descends it decreases the cost function stocastic gradient descent is actually a big number at every step it doesn't do any dissent it does not decrease the house functions we can see at every step those are the contours of the. Function sometimes it goes up sometimes it goes down it fluctuates around but it kind of sarcastically still seems to be making progress towards the optimum and stocastic gradient descent because it's not using ex. At gradients just working with these random examples it actually is much more sensitive to step sizes and you can see as the increase the step size its behavior this is like actually. Simulation for a toy problem so initially so what I want you to notice is let me go through this or you know a few times keep looking at what patterns you may notice in how. Line is fluctuating hopefully this is big enough for everybody to see or so this slider that I'm shifting is just the step size so let me just remind you in case you forgot the iteration we are running escapees. One is x k minus so tank its called alpa ter