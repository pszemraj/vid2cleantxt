WELL LET'S LET ME ACTUALLY TELL YOU THEN ABOUT RATHER THAN THE THE PROOF I THINK I'LL SHARE THE PROOF WITH GILL BECAUSE THE PROOF THAT I WANT WANTED TO ACTUALLY SHOW YOU GIVES A PROOF OF
 THE STOCASTIC GRADIENT IS WELL BEHAVED ON BOTH CONVEX AND NON CONVEX PROBLEMS AND THE PROOF I WANTED TO SHOW WAS FOR THE NON CONVEX CASE BECAUSE IT APPLIES TO NEURAL NETWORK SO YOU MAY BE CURIOUS ABOUT THAT PROOF AND REMARKABLY THAT PROOF IS MUCH SIMPLER THAN THE CASE OF CONVEX
 PROBLEMS SO LET ME JUST MENTION SOME VERY IMPORTANT POINTS ABOUT SARCASTIC RADIENT SO EVEN THOUGH THIS METHOD HAS BEEN AROUND SINCE NINETEEN FIFTY ONE EVERY DEEP LEARNING TOOLKIT HAS IT AND WE ARE STUDYING IT IN CLASS THERE ARE STILL GAPSBET
 IN WHAT WE CAN SAY THEORETICALLY AND WHAT HAPPENS IN PRACTICE AND I'LL SHOW YOU THOSE GAPS ALREADY AND ENCOURAGE YOU TO THINK ABOUT THOSE IF YOU WISH SO LET'S LOOK BACK AT OUR PROBLEM AND TELL YOU ABOUT TWO VARIANTS SO HERE ARE THE TWO VARIANTS I'M GOING
 ASK IF ANY OF YOU IS FAMILIAR WITH THESE VARIANTS IN SOME WAY OR OTHER SO LET'S I JUST CALLED IT FEASIBLE HERE THERE ARE NO CONSTRAINTS SO YOU START WITH ANY RANDOM FACTOR OF YOUR CHOICE IN DEEP NETWORK TRAIN
 YOU HAVE TO WORK HARDER AND THEN THIS IS THE ITERATION YOU RUN RIGHT OPTION ONE AND OPTION TWO SO OPTION ONE SAYS THAT WAS THE IDEA WE HAD IN CLASS RANDOM LY PICK SOME TRAINING DATA POINT USE ITS DORCASTIC GRADIENT
 WELL WHAT DO WE MEAN BY RANDOM LY PICK THE MOMENT YOU USE THE WORD RANDOM YOU HAVE TO DEFINE WHAT'S THE RANDOM NESS SO ONE RANDOM NESS IS UNIFORM PROBABILITY
 AND TRAINING DATA POINTS THAT IS ONE'S RANDOMNESS THE OTHER VERSION IS YOU PICK A TRAINING DATA POINT WITHOUT REPLACEMENT SO SO WITH REPLACEMENT MEANS YOU KNOW JUST UNIFORMLY AT RANDOM EACH TIME
 DRAW A NUMBER FROM ONE THROUGH N USE THAT SARCASTIC RADIENT MOVE ON WHICH MEANS THE SAME POINT CAN EASILY BE PICKED TWICE ALSO AND WITHOUT REPLACEMENT MEANS IF YOU'VE PICKED U POINT NUMBER THREE YOU'RE NOT GOING TO PICK IT AGAIN UNTIL YOU'VE
 THROUGH THE ENTIRE TRAINING DATA SET THOSE ARE TWO TYPES OF RANDOMNESS WHICH VERSION WOULD YOU USE THERE IS NO RIGHT OR WRONG ANSWER TO THIS I'M JUST TAKING A POLE
 WHAT WOULD YOU USE THINK THAT YOU'RE WRITING A PROGRAMME FOR THIS AND MAYBE THINK REALLY PRAGMATICALLY PRACTICALLY THAT'S ENOUGH OF AN HIN WHICH VERSION WOULD YOU USE I'M JUST CURIOUS
 WHO WOULD USE ONE PLEASE RAISE HANDS O WHO AND THE THE EXCLUSION THE COMPLIMENT THEREOF OR I DON'T KNOW MAYBE SOME PEOPLE ARE UNDECIDED WHO WOULD USE TOO VERY FEW PEOPLE WO
 OK HOW MANY OF YOU USE THE NEURAL NETWORK TRAINING TOOL KITS LIKE DENSE FLO PIT TORCH WHAT NOT WHICH VERSION ARE THEY USING
 ACTUALLY EVERY PERSON IN THE REAL WORLD IS USING VERSION TOO ARE YOU REALLY GOING TO RANDOMLY GO THROUGH YOUR RAM EACH TIME TO PICK RANDOM POINTS THEY'LL KILL YOUR G P PERFORMANCE LIKE ANYTHING
 WHAT PEOPLE DO IS TAKE A DATA SET USE A PRE SHUFFLE OPERATION AND THEN JUST WHOOP STREAM THROUGH THE DATA WHAT DOES STREAMING THROUGH THE DATA MEAN WITHOUT REPLACEMENT SO ALL THE TOOLKITS ACTUALLY ARE USING THE WITHOUT REPLACEMENT ER
 EVEN THOUGH INTUITIVELY THE RE JUS UNIFORM RANDOM FEELS MUCH NICER AND THAT FEELING IS NOT ILL FOUNDED BECAUSE THAT'S THE ONLY VERSION WE KNOW HOW TO ANALYZE MATHEMATICALLY SO EVEN FOR THIS METHOD EVERYBODY STUDIES IT THERE
 AN PAPERS ON IT THE VERSION THAT IS USED IN PRACTICE IS NOT THE VERSION WE KNOW HOW TO ANALYZE IT'S A MAJOR OPEN PROBLEM IN THE FIELD OF STOCASTIC GRADIENT TO ACTUALLY ANALYZE THE VERSION THAT WE USE IN PRACTICE THIS KIND OF
 RISING BUT IT'S WITHOUT REPLACEMENT MEANS NON IID PROBABILITY THEORY AND NON IIDE PROBABILITY THEORY IS NOT SO EASY THAT'S THE ANSWER O K SO THE OTHER VERSION IS THIS MINE BATCH IDEA WHICH
 MENTIONED REALLY EARLY ON IS THAT RATHER THAN PICK ONE RANDOM POINT I'LL PICK A MINNIE BATCH SO I HAD A MILLION POINTS EACH TIME INSTEAD OF PICKING ONE MAYBE I'LL PICK
 TEN OR HUNDRED OR THOUSAND OR WHAT HAVE YOU SO THIS AVERAGES THINGS AVERAGING THINGS REDUCES THE VARIANTS SO THIS IS ACTUALLY A GOOD THING CAUSE THE MORE QUANTITIES YOU AVERAGE THE LESS NOISE YOU HAVE THATS
 KIND OF WHAT HAPPENS IN PROBABILITY RIGHT SO WE PICK A MINNIE BATCH AND THE SARCASTIC ESTIMATE NOW IS THIS YOU KNOW NOT JUST A SINGLE GRADIENT BUT AVERAGED OVER A MINNIE BATCH
 SO MINI BATCH OF SIZE ONE IS THE PURE VANILLA S G D MINNI BATCH OF SIZE N IS NOTHING OTHER THAN PURE GRADIENT DESCENT SOMETHING IN BETWEEN IS WHAT PEOPLE ACTUALLY USE AND AGAIN THE THEORETICAL ANALYSIS ONLY EX
 IF THE MINNIE BATCH IS PICKED WITH REPLACEMENT NOT WITHOUT REPLACEMENT SO ONE OF THE REASONS ACTUALLY A VERY IMPORTANT THING IN THEORY YOU DON'T GAIN TOO MUCH IN TERMS OF COMPUTATIONAL GAINS ON CONVERGENCE
 PEED BY USING MANY MATCHES BUT MINY BATCHES ARE REALLY CRUCIAL SPECIALLY IN YOUR DEEP LEARNING GPU STYLE TRAINING BECAUSE THEY ALLOW YOU TO DO THINGS IN PARALLEL EACH THREAD OR EACH CORE OR SUB
 OR OR SMALL CHIP OR WHAT HAVE YOU DEPENDING ON YOUR HARDWARE CAN BE WORKING WITH ONE SORCASTIC RADIANT SO MINNI BATCHES THE LARGER THE MINNIE BATCH THE MORE THINGS YOU CAN DO IN PARALLEL SO MINNI BATCHES ARE GREATLYEXPLOT
 BY PEOPLE TO ER GIVE YOU A CHEAP VERSION OF PARALELISM AND WHERE DOES THE PARALLELISM HAPPEN YOU CAN THINK THAT EACH CORE COMPUTES A SARCASTIC GRADIENT SO THE HARD PART IS NOT
 ADDING THESE THINGS UP AND MAKING THE UPDATE TO X THE HARD PART IS COMPUTING A SARCASTIC GRADIENT SO IF YOU CAN COMPUTE TEN THOUSAND OF THOSE IN PARALLEL BECAUSE YOU HAVE TEN THOUSAND CORES GREAT FOR YOU AND THAT'S THE REASON PEOPLE LOVE USING MANE
 ACHES BUT A NICE SIDE REMARK HERE THIS IS ALSO THIS BRINGS US CLOSE TO THE RESEARCH EDGE OF THINGS AGAIN THAT WELL YOU'D LOVE TO USE VERY LARGE MINNIE BADGES SO THAT YOU CAN FULLY MAX OUT ON THE PARA
 ISM AVAILABLE TO YOU RIGHT MAYBE YOU HAVE A MULTI GEP SYSTEM IF YOUARNO FRIENDS WITH ENVY DEAR GOGA I ONLY HAVE LIKE TWO GPS BUT DEPENDS ON HOW MANY GPS YOU HAVE YOU'D LIKE TO REALLY MAX OT ON PARALLELISM SO THAT YOU CAN REALLY
 UNCH THROUGH BIG DATA SETS AS FAST AS POSSIBLE BUT YOU KNOW WHAT HAPPENS WITH VERY LARGE MINNY BATCHES SIFYO HAVE VERY LARGE MINNI BATCHES STARCASTIC GRADIENT STARTS LOOKING MORE LIKE
 FULL GRADIENT DESCENT WHICH IS ALSO CALLED BACH GRADIENT DESCENTTHAT'S NOT A BAD THING THAT'S AWESOME FOR OPTIMIZATION BUT IT IS A WEIRD CONUNDRUM THAT HAPPENS IN TRAINING DEEP NERAL NETWORKS
 THIS TYPE OF PROBLEM WE WOULDN'T HAVE FOR CONVEX OPTIMIZATION BUT IN DEEP NURALET WECAUS THIS REALLY DISTURBING THING HAPPENS THAT IF YOU USE THESE VERY LARGE MANY BATCHES YOUR METHOD STARTS RESEMBLING RADIANT DESCENT THAT MEANS IT DECREASES NOISE
 MUCH SO THAT THIS REGION OF CONFUSION SHRINKS SO MUCH WHICH ALL SOUNDS GOOD BUT IT ENDS UP BEING REALLY BAD FOR MACHINE LEARNING THAT'S WHAT I SAID THAT IN MACHINE LEARNING YOU WANT SOME REGION OF UNCERTAINTY AND WHAT IT
 MEANS ACTUALLY IS A LOT OF PEOPLE HAVE BEEN WORKING ON THIS INCLUDING AT BIG COMPANIES THAT IF YOU REDUCE THAT REGION OF UNCERTAINTY TOO MUCH YOU END UP OVER FITTING YOUR NEURAL NETWORK
 AND THEN IT STARTS SUCKING IN ITS TEST DATA UNSEEN DATA PERFORMANCE SO EVEN THOUGH FOR PARALLELISM PROGRAMMING OPTIMIZATION THEORY BIG MINNI BATCH IS AWESOME
 TUNATELY THERE PRICE TO BE PAID THAT IT HURTS YOUR TEST ERROR PERFORMANCE AND THEY'RE ALL SORTS OF METHODS PEOPLE ARE TRYING TO COOK UP INCLUDING A SHRINKING ETA
 INGLY OR CHANGING NEURA NETWORK ARCHITECTURE AND ALL SORTS OF IDEAS YOU CAN COOK UP YOUR IDEAS FOR YOUR FAVOURITE ARCHITECTURE HOW TO MAKE A LARGE MINNIE BATCH WITHOUT HURTING THE FINAL PERFORMANCE BUT IT'S STILL SOMEWHAT OF AN OPEN QUESTION ON HOW TO OPTIMIL
 ECT WHICH MENI HOW LARGE YOUR MANY BATS SHOULD BE SO EVEN THOUGH THESE IDEAS ARE SIMPLE YOU SEE THAT EVERY SIMPLE IDEA LEADS TO AN ENTIRE SUB AREA OF S G D
 HERE ARE PRACTICAL CHALLENGES PEOPLE HAVE VARIOUS HERISTICS FOR SOLVING THESE CHALLENGES YOU CAN COOK UP YOUR OWN BUT IT'S NOT THAT ONE IDEA ALWAYS WORKS SO IF YOU LOOK AT S G D WHAT ARE THE
 IN PARTS THE MOVING PARTS IN GD THE GRADIENTS STACASTIC GRADIENTS THE STEP SIZE THE MINNI BATCH SO HOW SHOULD I PICK STEP SIZES VERY NON TRIVIAL PROBLEM DIFFERENT DEEP LEARNING TO
 S MAY HAVE DIFFERENT WAYS OF AUTOMATING THAT TUNING BUT IT'S ONE OF THE PAINFUL THINGS WHICH MINNY BATCH TO USE WITH REPLACEMENT WITHOUT REPLACEMENT I ALREADY SHOWED YOU BUT WHICH MINNY BATCH SHOULD I USE HOW LARGE THAT SHOULD BE AGAIN NOT
 EASY QUESTION TO ANSWER HOW TO COMPUTE STARCASTIC GRADIENTS DOES ANYBODY KNOW HOW STARCASTIC GRADIENTS ARE COMPUTED FOR DEEP NETWORK TRAINING ANYBODY KNOW THERE ISA
 FAMOUS ELGARITHM CALLED BACK PROPAGATION THAT BACK PROPAGATION ELGRITHM IS USED TO COMPUTE A SINGLE STARCASTIC GRADIENT SOME PEOPLE USE THE WORD BACK PROP TO MEAN ASJIDDY BUT WHAT BACK PROP REALLY MEANS IS SOME
 SOME KIND OF ALGORISM WHICH COMPUTES FOR YOU A SINGLE SARCASTIC GRADIENT AND HENCE YOUKNOW THIS R TENSER FLUED CETRA THESE TOOLKISS THEY COME UP WITH ALL SORTS OF WAST AUTOMATE THE COMPUTATION OF A GRADIENT BECAUSE REALLY THAT'S THE MAIN THING
 AND THEN OTHER IDEAS LIKE RADIENT CLIPPING AND MOMENTUM ET CETERA THE BUNCH OF OTHER IDEAS AND THE THEORETICAL CHALLENGES I MENTIONED TO YOU ALREADY PROVING THAT IT WORKS THAT IT ACTUALLY SOLVES WHAT IT SET OUT TO DO UNFORTUNATELY I WAS TOO SLOW
 COULDN'T SHOW YOU THE AWESOME FIVE LINE PROOF THAT I HAVE THAT S GD WORKS FOR NEWRAL NETWORKS AND THEORETICAL ANALYSIS AS I SAID IS REALLY LAGGING MY PROOF ALSO USES THE WID
 CEMENT AND THE WITHOUT REPLACEMENT VERSION WHICH IS THE ONE THAT IS ACTUALLY IMPLEMENTED O THERE'S VERY LITTLE PROGRESS ON THAT THERE IS SOME PROGRESS THERE'S A BUNCH OF PAPERS INCLUDING FROM OUR COLLEAGUES THAT IMIGHTY BUT IT'S QUITE
 AND THE BIGGEST QUESTION WHICH MOST OF THE PEOPLE IN MACHINE LEARNING ARE CURRENTLY EXCITED ABOUT THESE DAYS IS STUFF LIKE WHY DOES S GD WORK SO WELL FOR NEURAL NETWORKS WE
 THIS SCRAPPY OPTIMIZATION METHOD IT VERY RAPIDLY DOES SOME FITTING DATA IS LARGE NEURO NETWORK IS LARGE AND THEN THIS NEURO NETWORK ENDS UP HAVING GREAT CLASSIFICATION PERFORMANCE WHY IS THAT HAPPENING THAT'S CALLED TRYING TO EXPLAIN
 BUILD A THEORY OF GENERALIZATION WHY DOES AN S G D TRAINED NEURA NETWORK WORK BETTER THAN NEURAL NETWORKS TRAIN WITH MORE FANCY OPTIMIZATION METHODS IT'S A MYSTERY AND MOST OF THE PEOPLE WHO TAKE INTEREST IN THEORETICAL MACHINE LEARNING AND STATISTICS
 THAT IS ONE OF THE MYSTERIES THEY ARE TRYING TO UNDERSTAND SO I THINK THAT'S MY STORY OF S JIDDY AND THIS IS THE PART WE SCAP BUT ITS SOKE THE THE INTUITION BEHIND US JDDYS MUCH MORE IMPORTANT THAN THIS
 SO I THINK WE CAN THANK YOU CLOSE MAYBEWER THE ROOF OR MONDAYS EXACTLY I THINK SO THAT
 HEY'LL BE GREAT
